//  searchPagerank.c//  Assignment 2 COMP1927//  3 June 2016//  Created by Winnie Zheng && Vivian Bakiris#include <stdio.h>#include <stdlib.h>#include "string.h"#include "searchPagerank.h"#include "DLList.h"#include <sys/stat.h>#include <errno.h>//retrieves the size of a file in bytesoff_t fsize(const char *filename) {    struct stat st;        if (stat(filename, &st) == 0)        return st.st_size;        return -1;}//upper case to lowercasechar * normalize(char *line) {        int i;    for(i=0;i<=strlen(line);i++){        if(line[i]>=65&&line[i]<=90) {            line[i]=line[i]+32;        }    }    return line;}//remove grammar function - used to normalize in txt fileschar * removegrammar (char * str){     int p = 0;    for(p = 0; p<=(strlen(str));p++){        if(str[p]=='.' || str[p]==','){            str[p]='\0';        }            }        return str;    }DLList pageRankL (char *argv[], int argc) {            if(argc == 1) {        printf("nothing in command line lel\n");        return 0;    }        if(argc != 1) {        argc = argc-1;    }    DLList matchedUrls = newDLList();// this is what we will return    FILE *f =fopen("invertedIndex.txt", "r");       int count = 0;    int j = 1;    int k = 0;    char * urls = malloc(fsize("invertedIndex.txt"));      int test[argc];     int x = 1;    /***test before going through code: check all search words exist ***/    while(count < argc) {        f = fopen("invertedIndex.txt", "r");        k = 0;        while (fscanf(f,"%s", &urls[k]) != EOF) { //read through the file till the END of file            removegrammar(argv[x]);            normalize(argv[x]);            if((strcmp(argv[x], &urls[k]) == 0)) { //if we've found our search word                //  printf("k %s\n",argv[x]);                test[j] = 0;                break;            }            else {                test[j] = 1;            }            k++;        }        count++;        j++;                fclose(f);    }    j = 0;    while(j < argc) {        if(test[j] == 1) {            printf("no urls matching all search words\n");            return 0;        }        j++;    }        //if all our words entered on command line exist... proceed:    count = 0;    x = 1;    int c = 0;    int done = 0;     while(count < argc) { //loop to go through each search word in the search        f = fopen("invertedIndex.txt", "r");        k = 0;        c = 0;        DLList tempUrls = newDLList();        done = 0;               while (fscanf(f,"%s", &urls[k]) != EOF && done == 0) { //read through the file till the END of file            normalize(argv[x]); //normalize and remove grammar from our scanning process!            removegrammar(argv[x]);            if((strcmp(argv[x], &urls[k]) == 0)) { //if we've found our search word                while(fgets(&urls[k], 100, f) != NULL && done == 0) {//keep scaning through the urls to add in the list UNTIL we                    char * cpy = strchr(&urls[k], ' ');                    char *ptr = cpy;                    char *splitS;                                       //divide the line where we found our word into single url strings                    while(ptr != NULL && done == 0) {                        splitS = strsep(&ptr, " ");                        if(splitS[strlen(splitS)-1] == '\n') {                        splitS[strlen(splitS)-1] = '\0';                        }                        if(count == 0 && c != 0) {                            DLListAfter(matchedUrls, splitS);                        }                        else if(count > 0 && c!= 0) {                            DLListAfter(tempUrls, splitS); //store temp urls to compare/match later down this funciton.. (only for iterations > 0)                        }                        c++;                        if(ptr == NULL ) {                            done = 1;                            break; //exit loop as we've found all our urls needed for our current query word                                                            }                    }                                    }            }            k++;        }        //for query words >= 1, compare and match to find what urls they share in common        if(count > 0) {        matchedUrls = compareMatch(matchedUrls, tempUrls);        }        x++;        count++;        fclose(f);    }    //empty url match case    if(DLListLength(matchedUrls) == 0) {        printf("no matching urls for ya search words m8\n");        return 0;    }    return matchedUrls;}//if we have more htan one query term, check for common urls//list a = "matched url list" currently//list b = new word list.DLList compareMatch(DLList a, DLList b) {        //DLList match = newDLList(); //matched list of urls    DLListNode *curr = a->first; //node for first element of list a    DLListNode *curr2 =b->first;        //reset match markers - those unmatched will be deleted    while(curr!= NULL) {        curr->i = 0;        curr = curr->next;    }    curr = a->first;        while(curr2 != NULL) { //traverse through a to find matching elements to B        curr = a->first;        while(curr!= NULL) { //traver through B to find matching elements to A                   if(strcmp(curr2->data, curr->data) == 0){                               curr->i = 1; //marker for matching url                break;            }              curr = curr->next;        }        curr2 = curr2->next;    }        //delete urls that don't have a matching marker - aka not in our match url list    curr = a->first;    while(curr != NULL) {        a->curr = curr;        if(curr->i == 0) {            DLListDelete(a);        }        curr = curr->next;    }    return a;}void findPagerank (char *argv[], int argc) {       DLList urls = pageRankL(argv, argc);        if(urls == 0) { //if we have an empty url list gathered from query search        return; //return as there will be no page ranks    }        FILE *f = fopen("pagerankList.txt", "r");    char *temp = malloc(fsize("pagerankList.txt"));        DLList prUrls = newDLList();        int k = 0;        while (fscanf(f,"%s", &temp[k]) != EOF) { //scan through pagerank.txt and store each url in the urls array        int len = (int)(strlen(&temp[k])); //remove commas from extracting url name in pagerankList.txt        if(k % 3 == 0) { //take in every 2nd line - take in the url names and store in list            char *add = malloc(sizeof(char *) *len);            strncpy(&add[k], &temp[k],len-1);            add[strlen(add)+1] = '\0';            DLListAfter(prUrls, &add[k]);                   }                k++;    }    fclose(f);        int i = 0;    k = 0;            //now we have our array of PR in descending order... we compare and match and if we find it, we write it out to stdout    DLListNode * mUrl = urls->first;    DLListNode * prUrl = prUrls->first;    int maxUrls = 10;    int c = 0;        while(prUrl != NULL){        mUrl = urls->first;        while(mUrl!= NULL){ //traverse through all the urls                        if(strcmp(mUrl->data, prUrl->data) == 0) {                if(maxUrls+1 > c){                    printf("%s\n", prUrl->data); //print our url pages in descending order                                   c++;                }            }            mUrl = mUrl->next; //iterate            i++;        }                prUrl = prUrl->next; //iterate                k++;    }    }